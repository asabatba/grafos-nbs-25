{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc0dbdd",
   "metadata": {},
   "source": [
    "\n",
    "# Graph-Based Recommendation with NetworkX (3.x-safe)\n",
    "\n",
    "This notebook shows how to use graphs as simple recommendation engines:\n",
    "- Build a **user-item bipartite graph**\n",
    "- Create an **item-item projection** (co-occurrence strengths)\n",
    "- Score recommendations via **co-occurrence**, **Jaccard/Adamic-Adar** link prediction ideas, and **Personalized PageRank**\n",
    "- Run a tiny **offline evaluation** (Hit-Rate@K and MRR@K)\n",
    "- Visualize a few pieces with Matplotlib\n",
    "\n",
    "All code uses modern, non-deprecated NetworkX 3.x APIs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc30ede",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28bf823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you need packages, uncomment:\n",
    "# !pip install networkx matplotlib pandas numpy\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"NetworkX:\", nx.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47751cbf",
   "metadata": {},
   "source": [
    "## Create a tiny synthetic user-item dataset (üîÅ Customize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1771aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Users and items\n",
    "users = [f\"U{i}\" for i in range(1, 13)]\n",
    "items = [f\"I{i}\" for i in range(1, 17)]\n",
    "\n",
    "# Deterministic small dataset with overlapping tastes\n",
    "interactions = {\n",
    "    \"U1\":  [\"I1\",\"I2\",\"I3\",\"I7\"],\n",
    "    \"U2\":  [\"I2\",\"I3\",\"I4\",\"I8\"],\n",
    "    \"U3\":  [\"I1\",\"I3\",\"I5\",\"I9\"],\n",
    "    \"U4\":  [\"I4\",\"I5\",\"I6\",\"I10\"],\n",
    "    \"U5\":  [\"I2\",\"I6\",\"I7\",\"I11\"],\n",
    "    \"U6\":  [\"I1\",\"I5\",\"I7\",\"I12\"],\n",
    "    \"U7\":  [\"I8\",\"I9\",\"I10\",\"I13\"],\n",
    "    \"U8\":  [\"I3\",\"I9\",\"I11\",\"I14\"],\n",
    "    \"U9\":  [\"I6\",\"I10\",\"I12\",\"I15\"],\n",
    "    \"U10\": [\"I2\",\"I11\",\"I13\",\"I16\"],\n",
    "    \"U11\": [\"I4\",\"I8\",\"I12\",\"I14\"],\n",
    "    \"U12\": [\"I5\",\"I9\",\"I15\",\"I16\"],\n",
    "}\n",
    "\n",
    "# Convert to a DataFrame (implicit feedback: 1 per interaction)\n",
    "df = pd.DataFrame([(u, it, 1) for u, its in interactions.items() for it in its],\n",
    "                  columns=[\"user\",\"item\",\"weight\"])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc9c32",
   "metadata": {},
   "source": [
    "## Build a user-item bipartite graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e180238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_bipartite_graph(df, user_col=\"user\", item_col=\"item\", weight_col=\"weight\"):\n",
    "    G = nx.Graph(name=\"User-Item Graph\")\n",
    "    users = df[user_col].unique().tolist()\n",
    "    items = df[item_col].unique().tolist()\n",
    "    # Add nodes with bipartite attribute\n",
    "    G.add_nodes_from(users, bipartite=\"user\")\n",
    "    G.add_nodes_from(items, bipartite=\"item\")\n",
    "    # Add edges\n",
    "    for _, row in df.iterrows():\n",
    "        G.add_edge(row[user_col], row[item_col], weight=float(row.get(weight_col, 1.0)))\n",
    "    return G\n",
    "\n",
    "G = build_bipartite_graph(df)\n",
    "print(G)  # concise graph summary\n",
    "# Node counts by type\n",
    "n_users = sum(1 for n, d in G.nodes(data=True) if d.get(\"bipartite\") == \"user\")\n",
    "n_items = sum(1 for n, d in G.nodes(data=True) if d.get(\"bipartite\") == \"item\")\n",
    "print(\"Users:\", n_users, \"Items:\", n_items, \"Edges:\", G.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316599b4",
   "metadata": {},
   "source": [
    "## Item-item projection (co-occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfdd8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "item_nodes = [n for n, d in G.nodes(data=True) if d.get(\"bipartite\") == \"item\"]\n",
    "# Weighted projection: edge weight = number of shared users\n",
    "G_item = bipartite.weighted_projected_graph(G, item_nodes)\n",
    "print(G_item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff01c92",
   "metadata": {},
   "source": [
    "## Item-item similarity via Jaccard & Adamic-Adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f861c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from networkx.algorithms.link_prediction import jaccard_coefficient, adamic_adar_index\n",
    "\n",
    "# Precompute Jaccard and Adamic-Adar for item pairs only\n",
    "def pair_key(a, b):\n",
    "    # Undirected pair key for dict lookups\n",
    "    return tuple(sorted((a, b)))\n",
    "\n",
    "item_pairs = list(itertools.combinations(item_nodes, 2))\n",
    "\n",
    "jacc = {}\n",
    "for (u, v, p) in jaccard_coefficient(G, item_pairs):\n",
    "    jacc[pair_key(u, v)] = p\n",
    "\n",
    "aa = {}\n",
    "for (u, v, p) in adamic_adar_index(G, item_pairs):\n",
    "    aa[pair_key(u, v)] = p\n",
    "\n",
    "# Peek a few scores\n",
    "sample = list(jacc.items())[:10]\n",
    "sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72d746",
   "metadata": {},
   "source": [
    "## Recommender functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67150c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def user_items(G, user):\n",
    "    '''Return the set of items a user has interacted with.'''\n",
    "    return {nbr for nbr in G.neighbors(user) if G.nodes[nbr].get(\"bipartite\") == \"item\"}\n",
    "\n",
    "def candidate_items(G, user):\n",
    "    '''Items the user has NOT interacted with yet.'''\n",
    "    have = user_items(G, user)\n",
    "    items = {n for n, d in G.nodes(data=True) if d.get(\"bipartite\") == \"item\"}\n",
    "    return list(items - have)\n",
    "\n",
    "def recommend_cooccurrence(G, G_item, user, topk=10):\n",
    "    '''Score candidates by summing item-item co-occurrence weights to the user's items.'''\n",
    "    owned = user_items(G, user)\n",
    "    cand = candidate_items(G, user)\n",
    "    scores = {}\n",
    "    for c in cand:\n",
    "        s = 0.0\n",
    "        for o in owned:\n",
    "            w = G_item.get_edge_data(c, o, default={\"weight\": 0.0})[\"weight\"] if G_item.has_node(c) and G_item.has_node(o) else 0.0\n",
    "            s += float(w)\n",
    "        scores[c] = s\n",
    "    recs = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:topk]\n",
    "    return pd.DataFrame(recs, columns=[\"item\", \"score\"])\n",
    "\n",
    "def recommend_jaccard(G, user, jacc, agg=\"mean\", topk=10):\n",
    "    '''Score candidates by aggregating Jaccard scores to user's items (mean or max).'''\n",
    "    owned = list(user_items(G, user))\n",
    "    cand = candidate_items(G, user)\n",
    "    scores = {}\n",
    "    for c in cand:\n",
    "        vals = [jacc.get(tuple(sorted((c, o))), 0.0) for o in owned]\n",
    "        val = max(vals) if agg == \"max\" else (sum(vals) / len(vals) if vals else 0.0)\n",
    "        scores[c] = float(val)\n",
    "    recs = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:topk]\n",
    "    return pd.DataFrame(recs, columns=[\"item\", \"score\"])\n",
    "\n",
    "def recommend_adamic_adar(G, user, aa, agg=\"sum\", topk=10):\n",
    "    '''Score candidates by aggregating Adamic-Adar scores to user's items (sum or mean).'''\n",
    "    owned = list(user_items(G, user))\n",
    "    cand = candidate_items(G, user)\n",
    "    scores = {}\n",
    "    for c in cand:\n",
    "        vals = [aa.get(tuple(sorted((c, o))), 0.0) for o in owned]\n",
    "        if agg == \"mean\":\n",
    "            val = (sum(vals) / len(vals)) if vals else 0.0\n",
    "        else:\n",
    "            val = sum(vals)\n",
    "        scores[c] = float(val)\n",
    "    recs = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:topk]\n",
    "    return pd.DataFrame(recs, columns=[\"item\", \"score\"])\n",
    "\n",
    "def recommend_ppr(G, user, topk=10, alpha=0.85):\n",
    "    '''Personalized PageRank from the user node; return top items not yet seen.'''\n",
    "    if user not in G:\n",
    "        raise ValueError(f\"Unknown user: {user}\")\n",
    "    personalization = {user: 1.0}\n",
    "    pr = nx.pagerank(G, alpha=alpha, personalization=personalization)\n",
    "    owned = user_items(G, user)\n",
    "    items = [n for n, d in G.nodes(data=True) if d.get(\"bipartite\") == \"item\"]\n",
    "    candidates = [it for it in items if it not in owned]\n",
    "    recs = sorted(((it, pr.get(it, 0.0)) for it in candidates), key=lambda x: x[1], reverse=True)[:topk]\n",
    "    return pd.DataFrame(recs, columns=[\"item\",\"score\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a83feb",
   "metadata": {},
   "source": [
    "## Demo: recommendations for a single user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f973ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "u = \"U3\"\n",
    "print(\"User\", u, \"owns:\", sorted(user_items(G, u)))\n",
    "\n",
    "co_df  = recommend_cooccurrence(G, G_item, u, topk=10)\n",
    "jac_df = recommend_jaccard(G, u, jacc, agg=\"mean\", topk=10)\n",
    "aa_df  = recommend_adamic_adar(G, u, aa, agg=\"sum\", topk=10)\n",
    "ppr_df = recommend_ppr(G, u, topk=10, alpha=0.85)\n",
    "\n",
    "print(\"\\nCo-occurrence:\"); display(co_df)\n",
    "print(\"\\nJaccard (mean to owned):\"); display(jac_df)\n",
    "print(\"\\nAdamic-Adar (sum to owned):\"); display(aa_df)\n",
    "print(\"\\nPersonalized PageRank:\"); display(ppr_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015238d8",
   "metadata": {},
   "source": [
    "## Quick visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Degree distribution for items\n",
    "item_degs = [G.degree(i) for i in item_nodes]\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(item_degs, bins=range(1, 1+max(item_degs)))\n",
    "plt.title(\"Item degree distribution\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ea745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize a small subgraph of the item-item projection: top 20 strongest co-occurrences\n",
    "edges_sorted = sorted(G_item.edges(data=True), key=lambda e: e[2].get(\"weight\", 0.0), reverse=True)[:20]\n",
    "H = nx.Graph()\n",
    "H.add_nodes_from(item_nodes)\n",
    "H.add_edges_from([(u,v,{\"weight\":d.get(\"weight\", 0.0)}) for u,v,d in edges_sorted])\n",
    "\n",
    "pos = nx.spring_layout(H, seed=42)\n",
    "plt.figure(figsize=(7,5))\n",
    "nx.draw_networkx_nodes(H, pos, node_size=400)\n",
    "nx.draw_networkx_edges(H, pos, width=[H[u][v][\"weight\"] for u,v in H.edges()])\n",
    "nx.draw_networkx_labels(H, pos, font_size=9)\n",
    "plt.title(\"Top item-item co-occurrences (projection)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c93327f",
   "metadata": {},
   "source": [
    "## Tiny offline evaluation (Hit-Rate@K and MRR@K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_holdout(interactions, K=5):\n",
    "    rows = []\n",
    "    for u, its in interactions.items():\n",
    "        if len(its) < 2:\n",
    "            continue\n",
    "        hidden = its[-1]\n",
    "        train_its = its[:-1]\n",
    "        # Build training DF (replace the user's interactions with train-only)\n",
    "        rec_df = pd.DataFrame([(uu, it, 1) for uu, lst in interactions.items() for it in (lst if uu != u else train_its)],\n",
    "                              columns=[\"user\",\"item\",\"weight\"])\n",
    "        Gtr = build_bipartite_graph(rec_df)\n",
    "        item_nodes_tr = [n for n, d in Gtr.nodes(data=True) if d.get(\"bipartite\") == \"item\"]\n",
    "        Gtr_item = nx.algorithms.bipartite.weighted_projected_graph(Gtr, item_nodes_tr)\n",
    "\n",
    "        # Precompute sims on training graph\n",
    "        pairs_tr = list(itertools.combinations(item_nodes_tr, 2))\n",
    "        jacc_tr = {tuple(sorted((a,b))): s for (a,b,s) in nx.jaccard_coefficient(Gtr, pairs_tr)}\n",
    "        aa_tr   = {tuple(sorted((a,b))): s for (a,b,s) in nx.adamic_adar_index(Gtr, pairs_tr)}\n",
    "\n",
    "        # Get rankings\n",
    "        def rank_of(item, df):\n",
    "            arr = df[\"item\"].tolist()\n",
    "            return (arr.index(item) + 1) if item in arr else None\n",
    "\n",
    "        co    = recommend_cooccurrence(Gtr, Gtr_item, u, topk=K)\n",
    "        jac   = recommend_jaccard(Gtr, u, jacc_tr, agg=\"mean\", topk=K)\n",
    "        aad   = recommend_adamic_adar(Gtr, u, aa_tr, agg=\"sum\", topk=K)\n",
    "        ppr   = recommend_ppr(Gtr, u, topk=K, alpha=0.85)\n",
    "\n",
    "        for name, df_rec in [(\"cooccurrence\", co), (\"jaccard\", jac), (\"adamic_adar\", aad), (\"ppr\", ppr)]:\n",
    "            r = rank_of(hidden, df_rec)\n",
    "            hit = 1 if (r is not None and r <= K) else 0\n",
    "            mrr = (1.0 / r) if r is not None else 0.0\n",
    "            rows.append({\"user\": u, \"held_out\": hidden, \"method\": name, \"rank\": r, \"hit@K\": hit, \"mrr@K\": mrr})\n",
    "\n",
    "    results = pd.DataFrame(rows)\n",
    "    summary = results.groupby(\"method\")[[\"hit@K\",\"mrr@K\"]].mean().reset_index().sort_values(\"mrr@K\", ascending=False)\n",
    "    return results, summary\n",
    "\n",
    "results, summary = evaluate_holdout(interactions, K=5)\n",
    "print(\"Per-user results (first 10 rows):\")\n",
    "display(results.head(10))\n",
    "print(\"\\nSummary (averaged over users):\")\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(summary[\"method\"], summary[\"hit@K\"])\n",
    "plt.title(\"Hit-Rate@5 by method\")\n",
    "plt.xlabel(\"Method\")\n",
    "plt.ylabel(\"Hit-Rate@5\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d0325",
   "metadata": {},
   "source": [
    "\n",
    "## Next steps\n",
    "- Swap in your own interactions data (CSV of `user,item[,weight]`).\n",
    "- Try weighting edges by rating/recency, or filter by minimum interactions.\n",
    "- Add Random Walk with Restart or node2vec embeddings for items.\n",
    "- For larger datasets, precompute projections/similarities offline and cache them.\n",
    "- Evaluate more formally (AUC, Recall@K, NDCG@K) using stronger splits.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

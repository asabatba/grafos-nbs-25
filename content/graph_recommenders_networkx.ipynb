{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f52752e",
   "metadata": {},
   "source": [
    "# Graph-Based Recommendation with NetworkX (3.x-safe)\n",
    "\n",
    "Graphs are a natural way to think about recommendation problems because users, items, and their interactions already form edges of a network. In this short workshop we walk from raw interactions through several graph-based recommenders.\n",
    "\n",
    "**Session duration:** ~45 minutes\n",
    "\n",
    "**Learning objectives**\n",
    "\n",
    "- Understand how a user-item bipartite graph supports recommendation tasks.\n",
    "- Build an item-item projection and compute lightweight similarity scores.\n",
    "- Compare graph-based strategies (co-occurrence, Jaccard, Adamic-Adar, Personalized PageRank) against a simple popularity baseline.\n",
    "- Evaluate recommendation quality with leave-one-out metrics and interpret the results.\n",
    "\n",
    "**Suggested timeline**\n",
    "\n",
    "1. Warm-up and notebook tour (5 min)\n",
    "2. Build the synthetic dataset and explore it (10 min)\n",
    "3. Construct the graphs and discuss similarity heuristics (15 min)\n",
    "4. Demo recommendations and hands-on exercises (10 min)\n",
    "5. Quick evaluation recap and wrap-up (5 min)\n",
    "\n",
    "> Tip for instructors: keep the code cells executed ahead of time if the audience is new to notebooks, but invite volunteers to run the \"Your turn\" prompts live.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10f6d52",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook assumes Python 3.11+ and the packages pinned in `requirements.txt`.\n",
    "\n",
    "- Run `pip install -r requirements.txt` once per environment.\n",
    "- Restart the kernel if you upgrade `networkx` so cached results do not leak across runs.\n",
    "- The imports below also configure pandas and matplotlib for inline plotting.\n",
    "\n",
    "If you are running this during the live session, clear all outputs ahead of time so the audience can see results appear cell by cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da33a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "pd.options.display.float_format = \"{:.3f}\".format\n",
    "try:\n",
    "    plt.style.use(\"seaborn-v0_8\")\n",
    "except OSError:\n",
    "    plt.style.use(\"seaborn-colorblind\")\n",
    "\n",
    "print(\"NetworkX:\", nx.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b0e33",
   "metadata": {},
   "source": [
    "## Dataset: synthetic user-item interactions\n",
    "\n",
    "We fabricate a compact dataset that still exposes meaningful structure for discussion. Each user has interacted with four items. Items are grouped into loose thematic clusters so we can talk about \"similar taste\" later on.\n",
    "\n",
    "We also attach a miniature catalog table that we can merge back into recommendations to make the results easier to narrate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2850f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users and items\n",
    "users = [f\"U{i}\" for i in range(1, 13)]\n",
    "item_catalog = pd.DataFrame(\n",
    "    [\n",
    "        (\"I1\", \"Graph Theory\"),\n",
    "        (\"I2\", \"Graph Theory\"),\n",
    "        (\"I3\", \"Graph Theory\"),\n",
    "        (\"I4\", \"Graph Theory\"),\n",
    "        (\"I5\", \"Visualization\"),\n",
    "        (\"I6\", \"Visualization\"),\n",
    "        (\"I7\", \"Visualization\"),\n",
    "        (\"I8\", \"Visualization\"),\n",
    "        (\"I9\", \"Machine Learning\"),\n",
    "        (\"I10\", \"Machine Learning\"),\n",
    "        (\"I11\", \"Machine Learning\"),\n",
    "        (\"I12\", \"Machine Learning\"),\n",
    "        (\"I13\", \"Data Platforms\"),\n",
    "        (\"I14\", \"Data Platforms\"),\n",
    "        (\"I15\", \"Data Platforms\"),\n",
    "        (\"I16\", \"Data Platforms\"),\n",
    "    ],\n",
    "    columns=[\"item\", \"category\"],\n",
    ")\n",
    "items = item_catalog[\"item\"].tolist()\n",
    "\n",
    "# Deterministic small dataset with overlapping tastes\n",
    "interactions = {\n",
    "    \"U1\": [\"I1\", \"I2\", \"I3\", \"I7\"],\n",
    "    \"U2\": [\"I2\", \"I3\", \"I4\", \"I8\"],\n",
    "    \"U3\": [\"I1\", \"I3\", \"I5\", \"I9\"],\n",
    "    \"U4\": [\"I4\", \"I5\", \"I6\", \"I10\"],\n",
    "    \"U5\": [\"I2\", \"I6\", \"I7\", \"I11\"],\n",
    "    \"U6\": [\"I1\", \"I5\", \"I7\", \"I12\"],\n",
    "    \"U7\": [\"I8\", \"I9\", \"I10\", \"I13\"],\n",
    "    \"U8\": [\"I3\", \"I9\", \"I11\", \"I14\"],\n",
    "    \"U9\": [\"I6\", \"I10\", \"I12\", \"I15\"],\n",
    "    \"U10\": [\"I2\", \"I11\", \"I13\", \"I16\"],\n",
    "    \"U11\": [\"I4\", \"I8\", \"I12\", \"I14\"],\n",
    "    \"U12\": [\"I5\", \"I9\", \"I15\", \"I16\"],\n",
    "}\n",
    "\n",
    "# Convert to a DataFrame (implicit feedback: 1 per interaction)\n",
    "df = pd.DataFrame(\n",
    "    [(u, it, 1) for u, its in interactions.items() for it in its],\n",
    "    columns=[\"user\", \"item\", \"weight\"],\n",
    ")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959bdd4f",
   "metadata": {},
   "source": [
    "### Inspect the interactions\n",
    "\n",
    "We keep the dataset tiny on purpose so you can trace recommendations back to individual co-occurrences during the session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea01a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Users: {len(users)}  Items: {len(items)}  Interactions: {len(df)}\")\n",
    "\n",
    "user_counts = (\n",
    "    df.groupby(\"user\")[\"item\"].count().rename(\"items_per_user\").sort_values(ascending=False)\n",
    ")\n",
    "item_counts = (\n",
    "    df.groupby(\"item\")[\"user\"].count().rename(\"users_per_item\").sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Items per user (should stay balanced in this synthetic set):\")\n",
    "display(user_counts)\n",
    "\n",
    "print(\"Users per item:\")\n",
    "display(item_counts)\n",
    "\n",
    "print(\"Sample interactions with item metadata:\")\n",
    "display(\n",
    "    df.merge(item_catalog, on=\"item\", how=\"left\")\n",
    "    .sample(5, random_state=42)\n",
    "    .sort_values([\"user\", \"item\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0b277",
   "metadata": {},
   "source": [
    "> **Your turn:** change the `interactions` dictionary to mirror your domain (movies, courses, internal documents) and re-run the next cells. Watch how the graph statistics react.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890039d6",
   "metadata": {},
   "source": [
    "## Build a user-item bipartite graph\n",
    "\n",
    "A bipartite graph separates users and items into two disjoint node sets. NetworkX lets us tag nodes with a `bipartite` attribute so we can keep the types apart.\n",
    "\n",
    "Key talking points:\n",
    "\n",
    "- Edges represent interactions (implicit feedback in this toy dataset).\n",
    "- The degree of a user is the number of items they touched (and vice versa for items).\n",
    "- We can already spot heavy users or popular items before computing any similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bipartite_graph(df, user_col=\"user\", item_col=\"item\", weight_col=\"weight\"):\n",
    "    '''Return a NetworkX Graph with bipartite labels.'''\n",
    "    G = nx.Graph(name=\"User-Item Graph\")\n",
    "    users_local = df[user_col].unique().tolist()\n",
    "    items_local = df[item_col].unique().tolist()\n",
    "    G.add_nodes_from(users_local, bipartite=\"user\")\n",
    "    G.add_nodes_from(items_local, bipartite=\"item\")\n",
    "    for _, row in df.iterrows():\n",
    "        G.add_edge(row[user_col], row[item_col], weight=float(row.get(weight_col, 1.0)))\n",
    "    return G\n",
    "\n",
    "\n",
    "G = build_bipartite_graph(df)\n",
    "user_nodes = [n for n, d in G.nodes(data=True) if d.get(\"bipartite\") == \"user\"]\n",
    "item_nodes = [n for n, d in G.nodes(data=True) if d.get(\"bipartite\") == \"item\"]\n",
    "\n",
    "density = bipartite.density(G, user_nodes)\n",
    "\n",
    "print(G)\n",
    "print(f\"Users: {len(user_nodes)}  Items: {len(item_nodes)}  Edges: {G.number_of_edges()}\")\n",
    "print(f\"Bipartite density: {density:.3f}\")\n",
    "\n",
    "avg_user_degree = sum(dict(G.degree(user_nodes)).values()) / len(user_nodes)\n",
    "avg_item_degree = sum(dict(G.degree(item_nodes)).values()) / len(item_nodes)\n",
    "print(f\"Avg items per user: {avg_user_degree:.2f}\")\n",
    "print(f\"Avg users per item: {avg_item_degree:.2f}\")\n",
    "\n",
    "example_user = user_nodes[0]\n",
    "print(f\"Example user {example_user} has items: {sorted(G.neighbors(example_user))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_degree = pd.Series(dict(G.degree(user_nodes)), name=\"degree\").sort_values(\n",
    "    ascending=False\n",
    ")\n",
    "item_degree = pd.Series(dict(G.degree(item_nodes)), name=\"degree\").sort_values(\n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "print(\"Top users by degree:\")\n",
    "display(user_degree.head(5).to_frame())\n",
    "\n",
    "print(\"Top items by degree:\")\n",
    "display(item_degree.head(5).to_frame())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea8183",
   "metadata": {},
   "source": [
    "## Item-item projection (co-occurrence)\n",
    "\n",
    "Projecting the bipartite graph onto the item set links two items when they share at least one user. The edge weight counts the number of shared users (co-occurrence frequency).\n",
    "\n",
    "This projection is the raw material for item-to-item recommenders such as Amazon's \"customers who bought X also bought Y\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fc43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_item = bipartite.weighted_projected_graph(G, item_nodes)\n",
    "\n",
    "print(G_item)\n",
    "print(f\"Projection edges: {G_item.number_of_edges()}\")\n",
    "\n",
    "edges_sorted = sorted(\n",
    "    G_item.edges(data=True), key=lambda e: e[2].get(\"weight\", 0.0), reverse=True\n",
    ")[:10]\n",
    "\n",
    "cooccurrence_table = pd.DataFrame(\n",
    "    [(a, b, data.get(\"weight\", 0.0)) for a, b, data in edges_sorted],\n",
    "    columns=[\"item_a\", \"item_b\", \"shared_users\"],\n",
    ")\n",
    "\n",
    "cooccurrence_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb315249",
   "metadata": {},
   "source": [
    "## Item-item similarity via Jaccard and Adamic-Adar\n",
    "\n",
    "Co-occurrence counts can be biased toward very popular items. Normalized similarity scores help rebalance the influence of hubs:\n",
    "\n",
    "- **Jaccard** divides shared users by the union of users who touched each item.\n",
    "- **Adamic-Adar** down-weights common neighbors that are themselves very popular.\n",
    "\n",
    "We pre-compute these scores for every item pair so we can reuse them across users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d230aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.link_prediction import adamic_adar_index, jaccard_coefficient\n",
    "\n",
    "pairs = list(itertools.combinations(item_nodes, 2))\n",
    "\n",
    "jacc = {tuple(sorted((a, b))): score for a, b, score in jaccard_coefficient(G, pairs)}\n",
    "\n",
    "aa = {tuple(sorted((a, b))): score for a, b, score in adamic_adar_index(G, pairs)}\n",
    "\n",
    "similarity_view = (\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            (a, b, jacc[tuple(sorted((a, b)))], aa[tuple(sorted((a, b)))])\n",
    "            for a, b in pairs\n",
    "        ],\n",
    "        columns=[\"item_a\", \"item_b\", \"jaccard\", \"adamic_adar\"],\n",
    "    )\n",
    "    .sort_values([\"jaccard\", \"adamic_adar\"], ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "similarity_view\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55f70c",
   "metadata": {},
   "source": [
    "## Recommender functions\n",
    "\n",
    "We will compare four simple strategies:\n",
    "\n",
    "1. **Popularity**: always recommend the most interacted items (non-graph baseline).\n",
    "2. **Co-occurrence**: sum item-item edge weights that connect to the user's history.\n",
    "3. **Jaccard**: use normalized overlap with existing items.\n",
    "4. **Adamic-Adar**: reward rare overlaps more than frequent ones.\n",
    "5. **Personalized PageRank**: random walks biased toward the user node.\n",
    "\n",
    "Feel free to experiment with the aggregation strategies or add your own heuristics (for example, weighting recent interactions more heavily).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ea2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_items(G, user):\n",
    "    \"\"\"Return the set of items a user has interacted with.\"\"\"\n",
    "    return {nbr for nbr in G.neighbors(user) if G.nodes[nbr].get(\"bipartite\") == \"item\"}\n",
    "\n",
    "\n",
    "def candidate_items(G, user):\n",
    "    \"\"\"Items the user has not interacted with yet.\"\"\"\n",
    "    owned = user_items(G, user)\n",
    "    items_all = {n for n, d in G.nodes(data=True) if d.get(\"bipartite\") == \"item\"}\n",
    "    return list(items_all - owned)\n",
    "\n",
    "\n",
    "def recommend_popularity(\n",
    "    df, user, topk=10, item_col=\"item\", user_col=\"user\", weight_col=\"weight\"\n",
    "):\n",
    "    \"\"\"Rank items by global popularity and filter out items already seen by the user.\"\"\"\n",
    "    owned = set(df[df[user_col] == user][item_col].tolist())\n",
    "    popularity = df.groupby(item_col)[weight_col].sum().sort_values(ascending=False)\n",
    "    recs = popularity.drop(labels=list(owned), errors=\"ignore\").head(topk)\n",
    "    return recs.reset_index().rename(columns={item_col: \"item\", weight_col: \"score\"})\n",
    "\n",
    "\n",
    "def recommend_cooccurrence(G, G_item, user, topk=10):\n",
    "    \"\"\"Score candidates by summing item-item co-occurrence weights.\"\"\"\n",
    "    owned = user_items(G, user)\n",
    "    cand = candidate_items(G, user)\n",
    "    scores = {}\n",
    "    for c in cand:\n",
    "        total = 0.0\n",
    "        for o in owned:\n",
    "            weight = (\n",
    "                G_item.get_edge_data(c, o, default={\"weight\": 0.0})[\"weight\"]\n",
    "                if G_item.has_node(c) and G_item.has_node(o)\n",
    "                else 0.0\n",
    "            )\n",
    "            total += float(weight)\n",
    "        scores[c] = total\n",
    "    recs = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:topk]\n",
    "    return pd.DataFrame(recs, columns=[\"item\", \"score\"])\n",
    "\n",
    "\n",
    "def recommend_jaccard(G, user, jacc, agg=\"mean\", topk=10):\n",
    "    \"\"\"Score candidates by aggregating Jaccard scores to the user's items.\"\"\"\n",
    "    owned = list(user_items(G, user))\n",
    "    cand = candidate_items(G, user)\n",
    "    scores = {}\n",
    "    for c in cand:\n",
    "        vals = [jacc.get(tuple(sorted((c, o))), 0.0) for o in owned]\n",
    "        if not vals:\n",
    "            scores[c] = 0.0\n",
    "            continue\n",
    "        scores[c] = max(vals) if agg == \"max\" else sum(vals) / len(vals)\n",
    "    recs = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:topk]\n",
    "    return pd.DataFrame(recs, columns=[\"item\", \"score\"])\n",
    "\n",
    "\n",
    "def recommend_adamic_adar(G, user, aa, agg=\"sum\", topk=10):\n",
    "    \"\"\"Score candidates by aggregating Adamic-Adar scores.\"\"\"\n",
    "    owned = list(user_items(G, user))\n",
    "    cand = candidate_items(G, user)\n",
    "    scores = {}\n",
    "    for c in cand:\n",
    "        vals = [aa.get(tuple(sorted((c, o))), 0.0) for o in owned]\n",
    "        if not vals:\n",
    "            scores[c] = 0.0\n",
    "            continue\n",
    "        if agg == \"mean\":\n",
    "            scores[c] = sum(vals) / len(vals)\n",
    "        else:\n",
    "            scores[c] = sum(vals)\n",
    "    recs = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:topk]\n",
    "    return pd.DataFrame(recs, columns=[\"item\", \"score\"])\n",
    "\n",
    "\n",
    "def recommend_ppr(G, user, topk=10, alpha=0.85):\n",
    "    \"\"\"Personalized PageRank from the user node; return top items not yet seen.\"\"\"\n",
    "    if user not in G:\n",
    "        raise ValueError(f\"Unknown user: {user}\")\n",
    "    personalization = {user: 1.0}\n",
    "    pr = nx.pagerank(G, alpha=alpha, personalization=personalization)\n",
    "    owned = user_items(G, user)\n",
    "    items_all = [n for n, d in G.nodes(data=True) if d.get(\"bipartite\") == \"item\"]\n",
    "    candidates = [it for it in items_all if it not in owned]\n",
    "    recs = sorted(\n",
    "        ((it, pr.get(it, 0.0)) for it in candidates), key=lambda x: x[1], reverse=True\n",
    "    )[:topk]\n",
    "    return pd.DataFrame(recs, columns=[\"item\", \"score\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a36bc1",
   "metadata": {},
   "source": [
    "### Compare recommenders for a sample user\n",
    "\n",
    "Below we inspect how the different strategies behave for one user. Look for overlap between the top results and relate them back to the original interactions or item categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c7b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_user = \"U3\"\n",
    "print(f\"User {demo_user} already has: {sorted(user_items(G, demo_user))}\")\n",
    "\n",
    "pop_df = recommend_popularity(df, demo_user, topk=5)\n",
    "co_df = recommend_cooccurrence(G, G_item, demo_user, topk=5)\n",
    "jac_df = recommend_jaccard(G, demo_user, jacc, agg=\"mean\", topk=5)\n",
    "aa_df = recommend_adamic_adar(G, demo_user, aa, agg=\"sum\", topk=5)\n",
    "ppr_df = recommend_ppr(G, demo_user, topk=5, alpha=0.85)\n",
    "\n",
    "for name, rec_df in [\n",
    "    (\"Popularity\", pop_df),\n",
    "    (\"Co-occurrence\", co_df),\n",
    "    (\"Jaccard (mean)\", jac_df),\n",
    "    (\"Adamic-Adar (sum)\", aa_df),\n",
    "    (\"Personalized PageRank\", ppr_df),\n",
    "]:\n",
    "    print(f\"\n",
    "{name} recommendations:\")\n",
    "    display(\n",
    "        rec_df.merge(item_catalog, on=\"item\", how=\"left\")\n",
    "        .assign(rank=lambda df_: range(1, len(df_) + 1))\n",
    "        [[\"rank\", \"item\", \"category\", \"score\"]]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1ec59",
   "metadata": {},
   "source": [
    "> **Your turn:** pick a different user (or edit their interactions) and re-run the cell above. Which strategies feel too conservative? Which ones surface more diverse content?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e3c6e4",
   "metadata": {},
   "source": [
    "## Quick visualizations\n",
    "\n",
    "Visuals help non-graph specialists understand why a recommendation appeared. We will look at degree distributions and a trimmed projection graph you can screenshot for slides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e87a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_degs = [G.degree(i) for i in item_nodes]\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(item_degs, bins=range(1, 1 + max(item_degs)))\n",
    "plt.title(\"Item degree distribution\")\n",
    "plt.xlabel(\"Number of users connected\")\n",
    "plt.ylabel(\"Count of items\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "pos = {}\n",
    "pos.update({u: (-1, idx) for idx, u in enumerate(sorted(user_nodes))})\n",
    "pos.update({it: (1, idx) for idx, it in enumerate(sorted(item_nodes))})\n",
    "\n",
    "nx.draw_networkx(\n",
    "    G,\n",
    "    pos=pos,\n",
    "    node_size=[300 if n in user_nodes else 400 for n in G.nodes()],\n",
    "    node_color=[\"#6baed6\" if n in user_nodes else \"#fd8d3c\" for n in G.nodes()],\n",
    "    with_labels=True,\n",
    "    edge_color=\"#cccccc\",\n",
    ")\n",
    "plt.title(\"User-item bipartite view\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c680a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_edges = sorted(\n",
    "    G_item.edges(data=True), key=lambda e: e[2].get(\"weight\", 0.0), reverse=True\n",
    ")[:20]\n",
    "\n",
    "H = nx.Graph()\n",
    "H.add_nodes_from(item_nodes)\n",
    "H.add_edges_from(\n",
    "    [(a, b, {\"weight\": data.get(\"weight\", 0.0)}) for a, b, data in top_edges]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "pos = nx.spring_layout(H, seed=42, weight=\"weight\")\n",
    "nx.draw_networkx_nodes(H, pos, node_size=400, node_color=\"#fd8d3c\")\n",
    "nx.draw_networkx_labels(H, pos)\n",
    "nx.draw_networkx_edges(\n",
    "    H,\n",
    "    pos,\n",
    "    width=[data[\"weight\"] for _, _, data in H.edges(data=True)],\n",
    "    edge_color=\"#999999\",\n",
    ")\n",
    "plt.title(\"Top item-item co-occurrences\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f4d62",
   "metadata": {},
   "source": [
    "## Tiny offline evaluation (Hit-Rate@K, MRR@K, Precision@K)\n",
    "\n",
    "We run a leave-one-out evaluation: hide the last interaction for each user, generate recommendations from the remaining history, and check where the hidden item ranks. This is not production-grade evaluation, but it is enough to compare heuristics in class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898aa665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_holdout(interactions, K=5):\n",
    "    rows = []\n",
    "    for u, its in interactions.items():\n",
    "        if len(its) < 2:\n",
    "            continue\n",
    "        hidden = its[-1]\n",
    "        train_its = its[:-1]\n",
    "        # Build training DF (replace the user's interactions with train-only)\n",
    "        rec_df = pd.DataFrame(\n",
    "            [\n",
    "                (uu, it, 1)\n",
    "                for uu, lst in interactions.items()\n",
    "                for it in (lst if uu != u else train_its)\n",
    "            ],\n",
    "            columns=[\"user\", \"item\", \"weight\"],\n",
    "        )\n",
    "        Gtr = build_bipartite_graph(rec_df)\n",
    "        user_nodes_tr = [n for n, d in Gtr.nodes(data=True) if d.get(\"bipartite\") == \"user\"]\n",
    "        item_nodes_tr = [n for n, d in Gtr.nodes(data=True) if d.get(\"bipartite\") == \"item\"]\n",
    "        Gtr_item = bipartite.weighted_projected_graph(Gtr, item_nodes_tr)\n",
    "\n",
    "        pairs_tr = list(itertools.combinations(item_nodes_tr, 2))\n",
    "        jacc_tr = {\n",
    "            tuple(sorted((a, b))): s for (a, b, s) in jaccard_coefficient(Gtr, pairs_tr)\n",
    "        }\n",
    "        aa_tr = {\n",
    "            tuple(sorted((a, b))): s for (a, b, s) in adamic_adar_index(Gtr, pairs_tr)\n",
    "        }\n",
    "\n",
    "        def rank_of(item, df_scores):\n",
    "            arr = df_scores[\"item\"].tolist()\n",
    "            return (arr.index(item) + 1) if item in arr else None\n",
    "\n",
    "        pop = recommend_popularity(rec_df, u, topk=K)\n",
    "        co = recommend_cooccurrence(Gtr, Gtr_item, u, topk=K)\n",
    "        jac = recommend_jaccard(Gtr, u, jacc_tr, agg=\"mean\", topk=K)\n",
    "        aad = recommend_adamic_adar(Gtr, u, aa_tr, agg=\"sum\", topk=K)\n",
    "        ppr = recommend_ppr(Gtr, u, topk=K, alpha=0.85)\n",
    "\n",
    "        for name, df_rec in [\n",
    "            (\"popularity\", pop),\n",
    "            (\"cooccurrence\", co),\n",
    "            (\"jaccard\", jac),\n",
    "            (\"adamic_adar\", aad),\n",
    "            (\"ppr\", ppr),\n",
    "        ]:\n",
    "            ranked = df_rec[\"item\"].tolist()\n",
    "            rank = rank_of(hidden, df_rec)\n",
    "            hit = 1 if (rank is not None and rank <= K) else 0\n",
    "            mrr = (1.0 / rank) if rank is not None else 0.0\n",
    "            precision = hit / K\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"user\": u,\n",
    "                    \"held_out\": hidden,\n",
    "                    \"method\": name,\n",
    "                    \"rank\": rank,\n",
    "                    \"hit@K\": hit,\n",
    "                    \"mrr@K\": mrr,\n",
    "                    \"precision@K\": precision,\n",
    "                    \"recommended\": tuple(ranked),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    results = pd.DataFrame(rows)\n",
    "    summary = (\n",
    "        results.groupby(\"method\")[ [\"hit@K\", \"mrr@K\", \"precision@K\"] ]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .sort_values(\"mrr@K\", ascending=False)\n",
    "    )\n",
    "    return results, summary\n",
    "\n",
    "\n",
    "results, summary = evaluate_holdout(interactions, K=5)\n",
    "print(\"Per-user results (first 10 rows):\")\n",
    "display(results.head(10))\n",
    "\n",
    "coverage = (\n",
    "    results.groupby(\"method\")[\"recommended\"]\n",
    "    .apply(lambda tuples: len(set(itertools.chain.from_iterable(tuples))))\n",
    "    .reset_index(name=\"unique_items\")\n",
    ")\n",
    "coverage[\"coverage@K\"] = coverage[\"unique_items\"] / len(item_nodes)\n",
    "\n",
    "summary_with_cov = summary.merge(coverage, on=\"method\")\n",
    "\n",
    "print(\"Summary (averaged over users):\")\n",
    "display(summary_with_cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5529e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].bar(summary_with_cov[\"method\"], summary_with_cov[\"hit@K\"], color=\"#3182bd\")\n",
    "axes[0].set_title(\"Hit-Rate@5 by method\")\n",
    "axes[0].set_xlabel(\"Method\")\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].tick_params(axis=\"x\", rotation=20)\n",
    "\n",
    "axes[1].bar(summary_with_cov[\"method\"], summary_with_cov[\"mrr@K\"], color=\"#9ecae1\")\n",
    "axes[1].set_title(\"MRR@5 by method\")\n",
    "axes[1].set_xlabel(\"Method\")\n",
    "axes[1].set_ylabel(\"Score\")\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].tick_params(axis=\"x\", rotation=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Coverage@5:\")\n",
    "display(summary_with_cov[[\"method\", \"coverage@K\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64864c2",
   "metadata": {},
   "source": [
    "## Facilitation prompts\n",
    "\n",
    "- Which metric best reflects success for your domain? Would recall or diversity matter more?\n",
    "- How could we blend graph-based scores with metadata such as `category`?\n",
    "- What happens if we include timestamps or weights on the edges?\n",
    "- How would you productionize this flow (batch jobs, feature stores, real-time serving)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9387945",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Swap in your own interactions data (CSV of `user,item[,weight]`) and re-run the exploration cells to sanity-check the degree distributions.\n",
    "- Try weighting edges by rating, recency, or time spent. Observe how it shifts the co-occurrence projection.\n",
    "- Add advanced algorithms such as Random Walk with Restart, node2vec embeddings, or graph neural networks once the team is comfortable with the basics.\n",
    "- Export the trained graphs to `dist/` and surface them on JupyterLite so attendees can experiment after the session.\n",
    "- Capture evaluation snapshots under `content/data/validation/` if you iterate on the heuristics.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
